# VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People

**Authors:** Daniel Killough¹, Justin Feng¹*, Zheng Xue "ZX" Ching¹*, Daniel Wang¹*, Rithvik Dyava¹*, Yapeng Tian², Yuhang Zhao¹

¹University of Wisconsin-Madison, ²University of Texas at Dallas\
*Authors 2-5 contributed equally to this work.

## 

**Links:** [Code](https://github.com/MadisonAbilityLab/VRSight) | Paper: [ArXiv Preprint!](https://arxiv.org/abs/2508.02958) | Videos: Coming Soon! | [HuggingFace Organization](https://huggingface.co/UWMadAbility)
<!-- %% Teaser Image %% -->

**VRSight** is a Computer Vision-powered VR system that leverages real-time object detection, and multimodal large language models to help support blind and low vision users access VR environments. 

This repo will soon hold: 
- VRSight code (system)
- DISCOVR (VR object detection dataset)
- DISCOVR-best.pt (best model weights)
  - Weights live on HuggingFace @ https://huggingface.co/UWMadAbility/VRSight/blob/main/best.pt
  - Note: Stored using pytorch default (pickle); safetensors version coming soon 
